{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### establishing connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vzD90kyGmPgV"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from snowflake.snowpark.session import Session\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "connection_parameters = {\n",
        "    \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
        "    \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n",
        "    \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
        "    \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
        "    \"database\": os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
        "    \"schema\": os.getenv(\"SNOWFLAKE_SCHEMA\"),\n",
        "    \"role\": os.getenv(\"SNOWFLAKE_ROLE\"),\n",
        "}\n",
        "\n",
        "snowpark_session = Session.builder.configs(connection_parameters).create()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### testing the connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLgI7HPinUaa",
        "outputId": "7643567a-e7ad-4557-a3fa-a41094cf5bd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " I can't directly view or understand images. However, if you describe the content of an image or provide textual information about it, I can help you analyze, interpret, or discuss that information. If you have any specific questions or need assistance with something related to an image, feel free to describe it, and I'll do my best to assist you!\n"
          ]
        }
      ],
      "source": [
        "from snowflake.cortex import Complete\n",
        "\n",
        "print(Complete(\"mistral-large2\", \"can you understand images ?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### building a simple cortex retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ChMH8p3bnn8x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from snowflake.core import Root\n",
        "from typing import List\n",
        "\n",
        "class CortexSearchRetriever:\n",
        "\n",
        "    def __init__(self, snowpark_session: Session, limit_to_retrieve: int = 3):\n",
        "        self._snowpark_session = snowpark_session\n",
        "        self._limit_to_retrieve = limit_to_retrieve\n",
        "\n",
        "    def retrieve(self, query: str) -> List[str]:\n",
        "        root = Root(self._snowpark_session)\n",
        "        cortex_search_service = (\n",
        "            root.databases[os.getenv(\"SNOWFLAKE_DATABASE\")]\n",
        "            .schemas[os.getenv(\"SNOWFLAKE_SCHEMA\")]\n",
        "            .cortex_search_services[os.getenv(\"SNOWFLAKE_CORTEX_SEARCH\")]\n",
        "        )\n",
        "        resp = cortex_search_service.search(\n",
        "            query=query,\n",
        "            columns=[\"TEXT_CONTENT\"],\n",
        "            limit=self._limit_to_retrieve,\n",
        "        )\n",
        "\n",
        "        if resp.results:\n",
        "            return [curr[\"TEXT_CONTENT\"] for curr in resp.results]\n",
        "        else:\n",
        "            return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### testing the retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UPNsohTmopxc"
      },
      "outputs": [],
      "source": [
        "retriever = CortexSearchRetriever(snowpark_session=snowpark_session, limit_to_retrieve=4)\n",
        "\n",
        "retrieved_context = retriever.retrieve(query=\"how was my christmas on December 25, 2024?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR7y6HWNoyDh",
        "outputId": "3dd443bc-4946-4e50-abed-ba83af687917"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\"This Christmas on December 25, 2024 was incredibly enjoyable for me. It had been almost six months since I last spent time with my family, as I had been away at college. Coming back for the holidays was special. We decorated the house together, exchanged gifts, and enjoyed a cozy dinner filled with laughter and catching up on each other\\'s lives. A funny moment was when my younger brother tried to surprise us with a homemade dessert, but he completely confused the sugar with salt, and the look on his face when we all took a bite was priceless! Despite the mishap, it added a lot of humor to the evening, and we still had an amazing time together.\"\\n(Note recorded on Tuesday, January 14, 2025 at 09:28 PM)',\n",
              " '\"On Tuesday, January 14, 2025, it was a typical busy day at university. After a couple of tough morning lectures, I headed to the library to work on an assignment. During our group project meeting, my friendâ€™s laptop crashed mid-presentation, and in a panic, they continued by scribbling notes on napkinsâ€”we all couldnâ€™t stop laughing! Later, I hit the gym for a quick workout before relaxing in my dorm with a movie. Overall, it was a hectic but fun day.\\n(Note recorded on Tuesday, January 14, 2025 at 09:29 PM)',\n",
              " '\"I\\'m planning to participate in the snowflake\\'s rag. I already have a product concept in mind, which Iâ€™ve named memex on Tuesday, January 14, 2025.\"\\n\\n(Note recorded on Tuesday, January 14, 2025 at 09:30 PM)',\n",
              " '\"I am planning to join the Snowflake hackathon. It is a rag-based hackathon. I have a product concept in mind called Memex on Tuesday, January 14, 2025.\"\\n\\n(conversation happened on Tuesday, January 14, 2025 at 10:40 PM)']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieved_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### instead of using actual dates in the query , often users might use reference dates. so creating a function to handle it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def get_current_date_info():\n",
        "    current = datetime.now()\n",
        "    return {\n",
        "        'date': current.strftime('%Y-%m-%d'),\n",
        "        'day': current.strftime('%A'),\n",
        "        'full_date': current.strftime('%A, %B %d, %Y'),\n",
        "        'time': current.strftime('%I:%M %p')\n",
        "    }\n",
        "def standardize_dates(query):\n",
        "  \n",
        "    current_date = get_current_date_info()\n",
        "  \n",
        "    prompt = f\"\"\"Current date is {current_date['full_date']}.\n",
        "    Convert any relative date references (today, tomorrow, next week, etc.) in this query to actual dates.if there is nothing relative, ignore the date provided, and just provide the query as it is.\n",
        "    Original query: \"{query}\"\n",
        "    Only output the converted query with no explanations or additional text.\"\"\"\n",
        "  \n",
        "    # Make LLM call using existing Snowflake Mixtral integration\n",
        "    response = Complete(\"mistral-large2\", prompt)\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' \"how was my christmas on December 25, 2024?\"'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "standard_query = standardize_dates(\"how was my christmas this year?\")\n",
        "standard_query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### starting trulens session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF8CsPnso15i",
        "outputId": "f4dafda8-8629-4201-dbcf-9508e5f356c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\OneDrive\\Desktop\\snowflake\\myenv\\lib\\site-packages\\trulens\\core\\utils\\imports.py:591: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  return self.imp(name, globals, locals, fromlist, level)\n",
            "Running the TruLens dashboard requires providing a `password` to the `SnowflakeConnector`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦‘ Initialized with db url snowflake://YUBRAJ:***@WPLDZCJ-MINDBOOK/MINDBOOKLM/DATA?role=ACCOUNTADMIN&warehouse=COMPUTE_WH .\n",
            "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Updating app_name and app_version in apps table: 0it [00:00, ?it/s]\n",
            "Updating app_id in records table: 0it [00:00, ?it/s]\n",
            "Updating app_json in apps table: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error setting TruLens workspace version tag: 000002 (0A000): Unsupported feature 'TAG'., check if you have enterprise version of Snowflake.\n"
          ]
        }
      ],
      "source": [
        "from trulens.core import TruSession\n",
        "from trulens.connectors.snowflake import SnowflakeConnector\n",
        "\n",
        "tru_snowflake_connector = SnowflakeConnector(snowpark_session=snowpark_session)\n",
        "\n",
        "tru_session = TruSession(connector=tru_snowflake_connector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### building simple rag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWPiEGgso_jt",
        "outputId": "7d7658e2-1bc1-4094-ccdf-211c583b1440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decorating <function RAG_from_scratch.retrieve_context at 0x000002A4FBDFE320>\n",
            "decorating <function RAG_from_scratch.generate_completion at 0x000002A4FBDFE3B0>\n",
            "decorating <function RAG_from_scratch.query at 0x000002A4FBDFDF30>\n",
            "adding method <class '__main__.RAG_from_scratch'> retrieve_context __main__\n",
            "adding method <class '__main__.RAG_from_scratch'> generate_completion __main__\n",
            "adding method <class '__main__.RAG_from_scratch'> query __main__\n"
          ]
        }
      ],
      "source": [
        "from trulens.apps.custom import instrument\n",
        "\n",
        "\n",
        "class RAG_from_scratch:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.retriever = CortexSearchRetriever(snowpark_session=snowpark_session, limit_to_retrieve=4)\n",
        "\n",
        "    @instrument\n",
        "    def retrieve_context(self, query: str) -> list:\n",
        "        \"\"\"\n",
        "        Retrieve relevant text from vector store.\n",
        "        \"\"\"\n",
        "        standard_query = standardize_dates(query)\n",
        "        return self.retriever.retrieve(standard_query)\n",
        "\n",
        "    @instrument\n",
        "    def generate_completion(self, query: str, context_str: list) -> str:\n",
        "        \"\"\"\n",
        "        Generate answer from context.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"You are a personal AI assistant who helps the user recall and elaborate on their past thoughts, plans, and discussions.\n",
        "        You have access to the user's personal notes and memories.\n",
        "\n",
        "        Context of previous discussions:\n",
        "        <context>\n",
        "        {context_str}\n",
        "        </context>\n",
        "\n",
        "        User's current question: {query}\n",
        "\n",
        "        Based on the context and your understanding, provide a helpful and precise response.\n",
        "        If the context directly addresses the question, use those details.\n",
        "        If not, respond based on the most relevant information available.\n",
        "        Always be supportive and sound like a trusted personal assistant.\n",
        "\n",
        "        Respond with a clear, natural text response. Do not use any special formatting or JSON structure.\n",
        "        \"\"\"\n",
        "        return Complete(\"mistral-large2\", prompt)\n",
        "\n",
        "    @instrument\n",
        "    def query(self, query: str) -> str:\n",
        "        context_str = self.retrieve_context(query)\n",
        "        return self.generate_completion(query, context_str)\n",
        "\n",
        "\n",
        "rag = RAG_from_scratch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### to evaluate our app, let's use the rag triad which consists of three evaluation functions i.e context relevance ,groundness and answer relevance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDTFhuNxpb0n",
        "outputId": "1e71d8a4-0148-4763-bdd6-5409319f89e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… In Groundedness, input source will be set to __record__.app.retrieve_context.rets[:].collect() .\n",
            "âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "âœ… In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
            "âœ… In Context Relevance, input context will be set to __record__.app.retrieve_context.rets[:] .\n",
            "âœ… In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
            "âœ… In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
          ]
        }
      ],
      "source": [
        "from trulens.providers.cortex.provider import Cortex\n",
        "from trulens.core import Feedback\n",
        "from trulens.core import Select\n",
        "import numpy as np\n",
        "\n",
        "provider = Cortex(\n",
        "    snowpark_session,\n",
        "    model_engine=\"mistral-large2\",\n",
        ")\n",
        "\n",
        "f_groundedness = (\n",
        "    Feedback(provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
        "    .on(Select.RecordCalls.retrieve_context.rets[:].collect())\n",
        "    .on_output()\n",
        ")\n",
        "\n",
        "f_context_relevance = (\n",
        "    Feedback(provider.context_relevance, name=\"Context Relevance\")\n",
        "    .on_input()\n",
        "    .on(Select.RecordCalls.retrieve_context.rets[:])\n",
        "    .aggregate(np.mean)\n",
        ")\n",
        "\n",
        "f_answer_relevance = (\n",
        "    Feedback(provider.relevance, name=\"Answer Relevance\")\n",
        "    .on_input()\n",
        "    .on_output()\n",
        "    .aggregate(np.mean)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTX6HtQVpikc",
        "outputId": "bfb17da9-c89c-409f-84de-387466cf4162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "instrumenting <class '__main__.RAG_from_scratch'> for base <class '__main__.RAG_from_scratch'>\n",
            "\tinstrumenting retrieve_context\n",
            "\tinstrumenting generate_completion\n",
            "\tinstrumenting query\n",
            "skipping base <class 'object'> because of class\n",
            "skipping base <class '__main__.CortexSearchRetriever'> because of class\n",
            "skipping base <class 'object'> because of class\n"
          ]
        }
      ],
      "source": [
        "from trulens.apps.custom import TruCustomApp\n",
        "\n",
        "tru_rag = TruCustomApp(\n",
        "    rag,\n",
        "    app_name=\"memex\",\n",
        "    app_version=\"simple\",\n",
        "    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### testing  our rag system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8Ze4iytMqfZN"
      },
      "outputs": [],
      "source": [
        "prompts = [\"how was my christmas this year ?\", \"anything ideas i had in mind for snowflake hackathon.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "F3fm3RIUqnvV",
        "outputId": "aa6f89b9-ebf9-4813-b1f1-ab063c7b1144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calling <function RAG_from_scratch.query at 0x000002A4FBDFDF30> with (<__main__.RAG_from_scratch object at 0x000002A4FBA22F80>, 'how was my christmas this year ?')\n",
            "calling <function RAG_from_scratch.retrieve_context at 0x000002A4FBDFE320> with (<__main__.RAG_from_scratch object at 0x000002A4FBA22F80>, 'how was my christmas this year ?')\n",
            "calling <function RAG_from_scratch.generate_completion at 0x000002A4FBDFE3B0> with (<__main__.RAG_from_scratch object at 0x000002A4FBA22F80>, 'how was my christmas this year ?', ['\"This Christmas on December 25, 2024 was incredibly enjoyable for me. It had been almost six months since I last spent time with my family, as I had been away at college. Coming back for the holidays was special. We decorated the house together, exchanged gifts, and enjoyed a cozy dinner filled with laughter and catching up on each other\\'s lives. A funny moment was when my younger brother tried to surprise us with a homemade dessert, but he completely confused the sugar with salt, and the look on his face when we all took a bite was priceless! Despite the mishap, it added a lot of humor to the evening, and we still had an amazing time together.\"\\n(Note recorded on Tuesday, January 14, 2025 at 09:28 PM)', '\"On Tuesday, January 14, 2025, it was a typical busy day at university. After a couple of tough morning lectures, I headed to the library to work on an assignment. During our group project meeting, my friendâ€™s laptop crashed mid-presentation, and in a panic, they continued by scribbling notes on napkinsâ€”we all couldnâ€™t stop laughing! Later, I hit the gym for a quick workout before relaxing in my dorm with a movie. Overall, it was a hectic but fun day.\\n(Note recorded on Tuesday, January 14, 2025 at 09:29 PM)', '\"I\\'m planning to participate in the snowflake\\'s rag. I already have a product concept in mind, which Iâ€™ve named memex on Tuesday, January 14, 2025.\"\\n\\n(Note recorded on Tuesday, January 14, 2025 at 09:30 PM)', '\"I am planning to join the Snowflake hackathon. It is a rag-based hackathon. I have a product concept in mind called Memex on Tuesday, January 14, 2025.\"\\n\\n(conversation happened on Tuesday, January 14, 2025 at 10:40 PM)'])\n",
            "calling <function RAG_from_scratch.query at 0x000002A4FBDFDF30> with (<__main__.RAG_from_scratch object at 0x000002A4FBA22F80>, 'anything ideas i had in mind for snowflake hackathon.')\n",
            "calling <function RAG_from_scratch.retrieve_context at 0x000002A4FBDFE320> with (<__main__.RAG_from_scratch object at 0x000002A4FBA22F80>, 'anything ideas i had in mind for snowflake hackathon.')\n",
            "calling <function RAG_from_scratch.generate_completion at 0x000002A4FBDFE3B0> with (<__main__.RAG_from_scratch object at 0x000002A4FBA22F80>, 'anything ideas i had in mind for snowflake hackathon.', ['\"I am planning to join the Snowflake hackathon. It is a rag-based hackathon. I have a product concept in mind called Memex on Tuesday, January 14, 2025.\"\\n\\n(conversation happened on Tuesday, January 14, 2025 at 10:40 PM)', '\"I\\'m planning to participate in the snowflake\\'s rag. I already have a product concept in mind, which Iâ€™ve named memex on Tuesday, January 14, 2025.\"\\n\\n(Note recorded on Tuesday, January 14, 2025 at 09:30 PM)', '\"On Tuesday, January 14, 2025, it was a typical busy day at university. After a couple of tough morning lectures, I headed to the library to work on an assignment. During our group project meeting, my friendâ€™s laptop crashed mid-presentation, and in a panic, they continued by scribbling notes on napkinsâ€”we all couldnâ€™t stop laughing! Later, I hit the gym for a quick workout before relaxing in my dorm with a movie. Overall, it was a hectic but fun day.\\n(Note recorded on Tuesday, January 14, 2025 at 09:29 PM)', '\"On Monday, January 20, 2025, I have an important meeting with the Khalti CTO. We will be talking about stock price prediction and how AI can be used there.\\n\\n(conversation happened on Tuesday, January 14, 2025 at 09:42 PM)'])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\OneDrive\\Desktop\\snowflake\\myenv\\lib\\site-packages\\trulens\\feedback\\llm_provider.py:1521: UserWarning: Failed to process and remove trivial statements. Proceeding with all statements.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "with tru_rag as recording:\n",
        "  for prompt in prompts:\n",
        "        rag.query(prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Answer Relevance</th>\n",
              "      <th>Context Relevance</th>\n",
              "      <th>Groundedness</th>\n",
              "      <th>latency</th>\n",
              "      <th>total_cost</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>app_name</th>\n",
              "      <th>app_version</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>memex</th>\n",
              "      <th>simple</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.208333</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.018141</td>\n",
              "      <td>0.119127</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Answer Relevance  Context Relevance  Groundedness  \\\n",
              "app_name app_version                                                      \n",
              "memex    simple                   0.75           0.208333           1.0   \n",
              "\n",
              "                       latency  total_cost  \n",
              "app_name app_version                        \n",
              "memex    simple       9.018141    0.119127  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tru_session.get_leaderboard()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The answer relevancy and groundness scores show that the response from our RAG system is highly relevant to the question and grounded (i.e., no hallucinations). However, the context relevance score is only 0.20, indicating poor retriever performance. It might be pulling in some unrelated context/information along with relevant chunks, but the ratio of unrelated content seems to be much higher than the related one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### since some non-relevant context are being pulled by the retriever , let's implement a guardrail using Trulens based on context relevance score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decorating <function context_filter.__call__.<locals>.wrapper at 0x000002A488BAE0E0>\n",
            "adding method <class '__main__.filtered_RAG_from_scratch'> retrieve_context __main__\n"
          ]
        }
      ],
      "source": [
        "from trulens.core.guardrails.base import context_filter\n",
        "\n",
        "f_context_relevance_score = Feedback(\n",
        "    provider.context_relevance, name=\"Context Relevance\"\n",
        ")\n",
        "\n",
        "\n",
        "class filtered_RAG_from_scratch(RAG_from_scratch):\n",
        "\n",
        "    @instrument\n",
        "    @context_filter(f_context_relevance_score, 0.65, keyword_for_prompt=\"query\")\n",
        "    def retrieve_context(self, query: str) -> list:\n",
        "        \"\"\"\n",
        "        Retrieve relevant text from vector store.\n",
        "        \"\"\"\n",
        "        standard_query = standardize_dates(query)\n",
        "        return self.retriever.retrieve(standard_query)\n",
        "\n",
        "\n",
        "filtered_rag = filtered_RAG_from_scratch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "instrumenting <class '__main__.filtered_RAG_from_scratch'> for base <class '__main__.filtered_RAG_from_scratch'>\n",
            "\tinstrumenting retrieve_context\n",
            "\tinstrumenting generate_completion\n",
            "\tinstrumenting query\n",
            "instrumenting <class '__main__.filtered_RAG_from_scratch'> for base <class '__main__.RAG_from_scratch'>\n",
            "\tinstrumenting retrieve_context\n",
            "\tinstrumenting generate_completion\n",
            "\tinstrumenting query\n",
            "skipping base <class 'object'> because of class\n",
            "skipping base <class '__main__.CortexSearchRetriever'> because of class\n",
            "skipping base <class 'object'> because of class\n"
          ]
        }
      ],
      "source": [
        "tru_filtered_rag = TruCustomApp(\n",
        "    filtered_rag,\n",
        "    app_name=\"memex\",\n",
        "    app_version=\"filtered\",\n",
        "    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts = [\"how was my christmas this year ?\", \"anything ideas i had in mind for snowflake hackathon.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calling <function RAG_from_scratch.query at 0x000002A4FBDFDF30> with (<__main__.filtered_RAG_from_scratch object at 0x000002A48729DD50>, 'how was my christmas this year ?')\n",
            "calling <function context_filter.__call__.<locals>.wrapper at 0x000002A488BAE0E0> with (<__main__.filtered_RAG_from_scratch object at 0x000002A48729DD50>, 'how was my christmas this year ?')\n",
            "calling <function RAG_from_scratch.generate_completion at 0x000002A4FBDFE3B0> with (<__main__.filtered_RAG_from_scratch object at 0x000002A48729DD50>, 'how was my christmas this year ?', ['\"This Christmas on December 25, 2024 was incredibly enjoyable for me. It had been almost six months since I last spent time with my family, as I had been away at college. Coming back for the holidays was special. We decorated the house together, exchanged gifts, and enjoyed a cozy dinner filled with laughter and catching up on each other\\'s lives. A funny moment was when my younger brother tried to surprise us with a homemade dessert, but he completely confused the sugar with salt, and the look on his face when we all took a bite was priceless! Despite the mishap, it added a lot of humor to the evening, and we still had an amazing time together.\"\\n(Note recorded on Tuesday, January 14, 2025 at 09:28 PM)'])\n",
            "calling <function RAG_from_scratch.query at 0x000002A4FBDFDF30> with (<__main__.filtered_RAG_from_scratch object at 0x000002A48729DD50>, 'anything ideas i had in mind for snowflake hackathon.')\n",
            "calling <function context_filter.__call__.<locals>.wrapper at 0x000002A488BAE0E0> with (<__main__.filtered_RAG_from_scratch object at 0x000002A48729DD50>, 'anything ideas i had in mind for snowflake hackathon.')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\OneDrive\\Desktop\\snowflake\\myenv\\lib\\site-packages\\trulens\\feedback\\llm_provider.py:1521: UserWarning: Failed to process and remove trivial statements. Proceeding with all statements.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calling <function RAG_from_scratch.generate_completion at 0x000002A4FBDFE3B0> with (<__main__.filtered_RAG_from_scratch object at 0x000002A48729DD50>, 'anything ideas i had in mind for snowflake hackathon.', ['\"I\\'m planning to participate in the snowflake\\'s rag. I already have a product concept in mind, which Iâ€™ve named memex on Tuesday, January 14, 2025.\"\\n\\n(Note recorded on Tuesday, January 14, 2025 at 09:30 PM)', '\"I am planning to join the Snowflake hackathon. It is a rag-based hackathon. I have a product concept in mind called Memex on Tuesday, January 14, 2025.\"\\n\\n(conversation happened on Tuesday, January 14, 2025 at 10:40 PM)'])\n"
          ]
        }
      ],
      "source": [
        "with tru_filtered_rag as recording:\n",
        "    for prompt in prompts:\n",
        "        filtered_rag.query(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Answer Relevance</th>\n",
              "      <th>Context Relevance</th>\n",
              "      <th>Groundedness</th>\n",
              "      <th>latency</th>\n",
              "      <th>total_cost</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>app_name</th>\n",
              "      <th>app_version</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">memex</th>\n",
              "      <th>filtered</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.137481</td>\n",
              "      <td>0.070880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>simple</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.208333</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.018141</td>\n",
              "      <td>0.119127</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Answer Relevance  Context Relevance  Groundedness  \\\n",
              "app_name app_version                                                      \n",
              "memex    filtered                 1.00           1.000000           1.0   \n",
              "         simple                   0.75           0.208333           1.0   \n",
              "\n",
              "                        latency  total_cost  \n",
              "app_name app_version                         \n",
              "memex    filtered     16.137481    0.070880  \n",
              "         simple        9.018141    0.119127  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tru_session.get_leaderboard()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now, the app is much better. The context relevance score has now imporved significantly from (0.20 to 1.00) with the use of Trulens guardrail. This means, only the relevant context are being pulled and used for generating the answer, improving the quality of our rag system (memex).."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
