{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### establishing connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vzD90kyGmPgV"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from snowflake.snowpark.session import Session\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "connection_parameters = {\n",
        "    \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
        "    \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n",
        "    \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
        "    \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
        "    \"database\": os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
        "    \"schema\": os.getenv(\"SNOWFLAKE_SCHEMA\"),\n",
        "    \"role\": os.getenv(\"SNOWFLAKE_ROLE\"),\n",
        "}\n",
        "\n",
        "snowpark_session = Session.builder.configs(connection_parameters).create()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### testing the connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLgI7HPinUaa",
        "outputId": "7643567a-e7ad-4557-a3fa-a41094cf5bd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " I can't directly view or understand images. However, I can help you with information, answer questions, or provide explanations based on text descriptions of images. If you describe an image to me, I can certainly assist you with that!\n"
          ]
        }
      ],
      "source": [
        "from snowflake.cortex import Complete\n",
        "\n",
        "print(Complete(\"mistral-large2\", \"can you understand images ?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### building a simple cortex retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ChMH8p3bnn8x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from snowflake.core import Root\n",
        "from typing import List\n",
        "\n",
        "class CortexSearchRetriever:\n",
        "\n",
        "    def __init__(self, snowpark_session: Session, limit_to_retrieve: int = 3):\n",
        "        self._snowpark_session = snowpark_session\n",
        "        self._limit_to_retrieve = limit_to_retrieve\n",
        "\n",
        "    def retrieve(self, query: str) -> List[str]:\n",
        "        root = Root(self._snowpark_session)\n",
        "        cortex_search_service = (\n",
        "            root.databases[os.getenv(\"SNOWFLAKE_DATABASE\")]\n",
        "            .schemas[os.getenv(\"SNOWFLAKE_SCHEMA\")]\n",
        "            .cortex_search_services[\"CC_SEARCH_SERVICE_CS\"]\n",
        "        )\n",
        "        resp = cortex_search_service.search(\n",
        "            query=query,\n",
        "            columns=[\"TEXT_CONTENT\"],\n",
        "            limit=self._limit_to_retrieve,\n",
        "        )\n",
        "\n",
        "        if resp.results:\n",
        "            return [curr[\"TEXT_CONTENT\"] for curr in resp.results]\n",
        "        else:\n",
        "            return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### testing the retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UPNsohTmopxc"
      },
      "outputs": [],
      "source": [
        "retriever = CortexSearchRetriever(snowpark_session=snowpark_session, limit_to_retrieve=4)\n",
        "\n",
        "retrieved_context = retriever.retrieve(query=\"how was my christmas on December 25, 2024?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR7y6HWNoyDh",
        "outputId": "3dd443bc-4946-4e50-abed-ba83af687917"
      },
      "outputs": [],
      "source": [
        "retrieved_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### instead of using actual dates in the query , often users might use reference dates. so creating a function to handle it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def get_current_date_info():\n",
        "    current = datetime.now()\n",
        "    return {\n",
        "        'date': current.strftime('%Y-%m-%d'),\n",
        "        'day': current.strftime('%A'),\n",
        "        'full_date': current.strftime('%A, %B %d, %Y'),\n",
        "        'time': current.strftime('%I:%M %p')\n",
        "    }\n",
        "def standardize_dates(query):\n",
        "  \n",
        "    current_date = get_current_date_info()\n",
        "  \n",
        "    prompt = f\"\"\"Current date is {current_date['full_date']}.\n",
        "    Convert any relative date references (today, tomorrow, next week, etc.) in this query to actual dates.if there is nothing relative, ignore the date provided, and just provide the query as it is.\n",
        "    Original query: \"{query}\"\n",
        "    Only output the converted query with no explanations or additional text.\"\"\"\n",
        "  \n",
        "    # Make LLM call using existing Snowflake Mixtral integration\n",
        "    response = Complete(\"mistral-large2\", prompt)\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "standard_query = standardize_dates(\"how was my christmas this year?\")\n",
        "standard_query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### starting trulens session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF8CsPnso15i",
        "outputId": "f4dafda8-8629-4201-dbcf-9508e5f356c1"
      },
      "outputs": [],
      "source": [
        "from trulens.core import TruSession\n",
        "from trulens.connectors.snowflake import SnowflakeConnector\n",
        "\n",
        "tru_snowflake_connector = SnowflakeConnector(snowpark_session=snowpark_session)\n",
        "\n",
        "tru_session = TruSession(connector=tru_snowflake_connector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### building simple rag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWPiEGgso_jt",
        "outputId": "7d7658e2-1bc1-4094-ccdf-211c583b1440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decorating <function RAG_from_scratch.retrieve_context at 0x000001E99DCED090>\n",
            "decorating <function RAG_from_scratch.generate_completion at 0x000001E99DCED000>\n",
            "decorating <function RAG_from_scratch.query at 0x000001E99DCECD30>\n",
            "adding method <class '__main__.RAG_from_scratch'> retrieve_context __main__\n",
            "adding method <class '__main__.RAG_from_scratch'> generate_completion __main__\n",
            "adding method <class '__main__.RAG_from_scratch'> query __main__\n"
          ]
        }
      ],
      "source": [
        "from trulens.apps.custom import instrument\n",
        "\n",
        "\n",
        "class RAG_from_scratch:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.retriever = CortexSearchRetriever(snowpark_session=snowpark_session, limit_to_retrieve=4)\n",
        "\n",
        "    @instrument\n",
        "    def retrieve_context(self, query: str) -> list:\n",
        "        \"\"\"\n",
        "        Retrieve relevant text from vector store.\n",
        "        \"\"\"\n",
        "        standard_query = standardize_dates(query)\n",
        "        return self.retriever.retrieve(standard_query)\n",
        "\n",
        "    @instrument\n",
        "    def generate_completion(self, query: str, context_str: list) -> str:\n",
        "        \"\"\"\n",
        "        Generate answer from context.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"You are a personal AI assistant who helps the user recall and elaborate on their past thoughts, plans, and discussions.\n",
        "        You have access to the user's personal notes and memories.\n",
        "\n",
        "        Context of previous discussions:\n",
        "        <context>\n",
        "        {context_str}\n",
        "        </context>\n",
        "\n",
        "        User's current question: {query}\n",
        "\n",
        "        Based on the context and your understanding, provide a helpful and precise response.\n",
        "        If the context directly addresses the question, use those details.\n",
        "        If not, respond based on the most relevant information available.\n",
        "        Always be supportive and sound like a trusted personal assistant.\n",
        "\n",
        "        Respond with a clear, natural text response. Do not use any special formatting or JSON structure.\n",
        "        \"\"\"\n",
        "        return Complete(\"mistral-large2\", prompt)\n",
        "\n",
        "    @instrument\n",
        "    def query(self, query: str) -> str:\n",
        "        context_str = self.retrieve_context(query)\n",
        "        return self.generate_completion(query, context_str)\n",
        "\n",
        "\n",
        "rag = RAG_from_scratch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### to evaluate our app, let's use the rag triad which consists of three evaluation functions i.e context relevance ,groundness and answer relevance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDTFhuNxpb0n",
        "outputId": "1e71d8a4-0148-4763-bdd6-5409319f89e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ In Groundedness, input source will be set to __record__.app.retrieve_context.rets[:].collect() .\n",
            "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "✅ In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
            "✅ In Context Relevance, input context will be set to __record__.app.retrieve_context.rets[:] .\n",
            "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
            "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
          ]
        }
      ],
      "source": [
        "from trulens.providers.cortex.provider import Cortex\n",
        "from trulens.core import Feedback\n",
        "from trulens.core import Select\n",
        "import numpy as np\n",
        "\n",
        "provider = Cortex(\n",
        "    snowpark_session,\n",
        "    model_engine=\"mistral-large2\",\n",
        ")\n",
        "\n",
        "f_groundedness = (\n",
        "    Feedback(provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
        "    .on(Select.RecordCalls.retrieve_context.rets[:].collect())\n",
        "    .on_output()\n",
        ")\n",
        "\n",
        "f_context_relevance = (\n",
        "    Feedback(provider.context_relevance, name=\"Context Relevance\")\n",
        "    .on_input()\n",
        "    .on(Select.RecordCalls.retrieve_context.rets[:])\n",
        "    .aggregate(np.mean)\n",
        ")\n",
        "\n",
        "f_answer_relevance = (\n",
        "    Feedback(provider.relevance, name=\"Answer Relevance\")\n",
        "    .on_input()\n",
        "    .on_output()\n",
        "    .aggregate(np.mean)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTX6HtQVpikc",
        "outputId": "bfb17da9-c89c-409f-84de-387466cf4162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "instrumenting <class '__main__.RAG_from_scratch'> for base <class '__main__.RAG_from_scratch'>\n",
            "\tinstrumenting retrieve_context\n",
            "\tinstrumenting generate_completion\n",
            "\tinstrumenting query\n",
            "skipping base <class 'object'> because of class\n",
            "skipping base <class '__main__.CortexSearchRetriever'> because of class\n",
            "skipping base <class 'object'> because of class\n"
          ]
        }
      ],
      "source": [
        "from trulens.apps.custom import TruCustomApp\n",
        "\n",
        "tru_rag = TruCustomApp(\n",
        "    rag,\n",
        "    app_name=\"memex\",\n",
        "    app_version=\"simple\",\n",
        "    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### testing  our rag system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8Ze4iytMqfZN"
      },
      "outputs": [],
      "source": [
        "prompts = [\"how was my christmas this year ?\", \"how was my last saturday\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "F3fm3RIUqnvV",
        "outputId": "aa6f89b9-ebf9-4813-b1f1-ab063c7b1144"
      },
      "outputs": [],
      "source": [
        "with tru_rag as recording:\n",
        "  for prompt in prompts:\n",
        "        rag.query(prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Answer Relevance</th>\n",
              "      <th>Context Relevance</th>\n",
              "      <th>Groundedness</th>\n",
              "      <th>latency</th>\n",
              "      <th>total_cost</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>app_name</th>\n",
              "      <th>app_version</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>memex</th>\n",
              "      <th>simple</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.8</td>\n",
              "      <td>8.154223</td>\n",
              "      <td>0.113728</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Answer Relevance  Context Relevance  Groundedness  \\\n",
              "app_name app_version                                                      \n",
              "memex    simple                    1.0           0.166667           0.8   \n",
              "\n",
              "                       latency  total_cost  \n",
              "app_name app_version                        \n",
              "memex    simple       8.154223    0.113728  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tru_session.get_leaderboard()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The answer relevancy and groundness scores show that the response from our RAG system is highly relevant to the question and grounded (i.e., no hallucinations). However, the context relevance score is only 0.16, indicating poor retriever performance. It might be pulling in some unrelated context/information along with relevant chunks, but the ratio of unrelated content seems to be much higher than the related one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### since some non-relevant context are being pulled by the retriever , let's implement a guardrail using Trulens based on context relevance score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decorating <function context_filter.__call__.<locals>.wrapper at 0x000001E9ACF7D990>\n",
            "adding method <class '__main__.filtered_RAG_from_scratch'> retrieve_context __main__\n"
          ]
        }
      ],
      "source": [
        "from trulens.core.guardrails.base import context_filter\n",
        "\n",
        "f_context_relevance_score = Feedback(\n",
        "    provider.context_relevance, name=\"Context Relevance\"\n",
        ")\n",
        "\n",
        "\n",
        "class filtered_RAG_from_scratch(RAG_from_scratch):\n",
        "\n",
        "    @instrument\n",
        "    @context_filter(f_context_relevance_score, 0.65, keyword_for_prompt=\"query\")\n",
        "    def retrieve_context(self, query: str) -> list:\n",
        "        \"\"\"\n",
        "        Retrieve relevant text from vector store.\n",
        "        \"\"\"\n",
        "        standard_query = standardize_dates(query)\n",
        "        return self.retriever.retrieve(standard_query)\n",
        "\n",
        "\n",
        "filtered_rag = filtered_RAG_from_scratch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "instrumenting <class '__main__.filtered_RAG_from_scratch'> for base <class '__main__.filtered_RAG_from_scratch'>\n",
            "\tinstrumenting retrieve_context\n",
            "\tinstrumenting generate_completion\n",
            "\tinstrumenting query\n",
            "instrumenting <class '__main__.filtered_RAG_from_scratch'> for base <class '__main__.RAG_from_scratch'>\n",
            "\tinstrumenting retrieve_context\n",
            "\tinstrumenting generate_completion\n",
            "\tinstrumenting query\n",
            "skipping base <class 'object'> because of class\n",
            "skipping base <class '__main__.CortexSearchRetriever'> because of class\n",
            "skipping base <class 'object'> because of class\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Object (of type list is a sequence containing more than one dictionary. Lookup by item or attribute `rets` is ambiguous. Use a lookup by index(es) or slice first to disambiguate.\n",
            "Object (of type list is a sequence containing more than one dictionary. Lookup by item or attribute `rets` is ambiguous. Use a lookup by index(es) or slice first to disambiguate.\n",
            "Object (of type list is a sequence containing more than one dictionary. Lookup by item or attribute `rets` is ambiguous. Use a lookup by index(es) or slice first to disambiguate.\n",
            "Object (of type list is a sequence containing more than one dictionary. Lookup by item or attribute `rets` is ambiguous. Use a lookup by index(es) or slice first to disambiguate.\n",
            "Object (of type list is a sequence containing more than one dictionary. Lookup by item or attribute `rets` is ambiguous. Use a lookup by index(es) or slice first to disambiguate.\n",
            "Object (of type list is a sequence containing more than one dictionary. Lookup by item or attribute `rets` is ambiguous. Use a lookup by index(es) or slice first to disambiguate.\n"
          ]
        }
      ],
      "source": [
        "tru_filtered_rag = TruCustomApp(\n",
        "    filtered_rag,\n",
        "    app_name=\"memex\",\n",
        "    app_version=\"filtered\",\n",
        "    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts = [\"how was my christmas this year ?\", \"how was my last saturday\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with tru_filtered_rag as recording:\n",
        "    for prompt in prompts:\n",
        "        filtered_rag.query(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\OneDrive\\Desktop\\snowflake\\myenv\\lib\\site-packages\\trulens\\feedback\\llm_provider.py:1521: UserWarning: Failed to process and remove trivial statements. Proceeding with all statements.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Answer Relevance</th>\n",
              "      <th>Context Relevance</th>\n",
              "      <th>Groundedness</th>\n",
              "      <th>latency</th>\n",
              "      <th>total_cost</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>app_name</th>\n",
              "      <th>app_version</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">memex</th>\n",
              "      <th>filtered</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>10.286708</td>\n",
              "      <td>0.073477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>simple</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>8.154223</td>\n",
              "      <td>0.113728</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Answer Relevance  Context Relevance  Groundedness  \\\n",
              "app_name app_version                                                      \n",
              "memex    filtered                  1.0           0.888889      1.000000   \n",
              "         simple                    1.0           0.250000      0.866667   \n",
              "\n",
              "                        latency  total_cost  \n",
              "app_name app_version                         \n",
              "memex    filtered     10.286708    0.073477  \n",
              "         simple        8.154223    0.113728  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tru_session.get_leaderboard()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now, the app is much better. The context relevance score has now imporved significantly from (0.25 to 0.88) with the use of Trulens guardrail. This means, only the relevant context are being pulled and used for generating the answer, improving the quality of our rag system (memex).."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
